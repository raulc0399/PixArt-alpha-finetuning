diff --git a/train_scripts/train_pixart_lora_hf.py b/train_scripts/train_pixart_lora_hf.py
index 96acf29..5c5f5f0 100644
--- a/train_scripts/train_pixart_lora_hf.py
+++ b/train_scripts/train_pixart_lora_hf.py
@@ -21,6 +21,7 @@ import os
 import random
 import shutil
 from pathlib import Path
+from typing import List, Union
 
 import datasets
 import numpy as np
@@ -290,6 +291,20 @@ def parse_args():
     parser.add_argument(
         "--use_8bit_adam", action="store_true", help="Whether or not to use 8-bit Adam from bitsandbytes."
     )
+    parser.add_argument(
+        "--use_dora",
+        action="store_true",
+        default=False,
+        help="Whether or not to use Dora. For more information, see"
+        " https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig.use_dora"
+    )
+    parser.add_argument(
+        "--use_rslora",
+        action="store_true",
+        default=False,
+        help="Whether or not to use RS Lora. For more information, see"
+        " https://huggingface.co/docs/peft/package_reference/lora#peft.LoraConfig.use_rslora"
+    )
     parser.add_argument(
         "--allow_tf32",
         action="store_true",
@@ -465,21 +480,29 @@ def main():
     # See Section 3.1. of the paper.
     max_length = 120
 
+    weight_dtype = torch.float32
+    if accelerator.mixed_precision == "fp16":
+        weight_dtype = torch.float16
+    elif accelerator.mixed_precision == "bf16":
+        weight_dtype = torch.bfloat16
+
     # Load scheduler, tokenizer and models.
-    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler")
-    tokenizer = T5Tokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision)
+    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder="scheduler", torch_dtype=weight_dtype)
+    tokenizer = T5Tokenizer.from_pretrained(args.pretrained_model_name_or_path, subfolder="tokenizer", revision=args.revision, torch_dtype=weight_dtype)
 
-    text_encoder = T5EncoderModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision)
+    text_encoder = T5EncoderModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="text_encoder", revision=args.revision, torch_dtype=weight_dtype)
+    text_encoder.requires_grad_(False)
+    text_encoder.to(accelerator.device)
 
-    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision, variant=args.variant)
+    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder="vae", revision=args.revision, variant=args.variant, torch_dtype=weight_dtype)
+    vae.requires_grad_(False)
+    vae.to(accelerator.device)
 
-    transformer = Transformer2DModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="transformer", torch_dtype=torch.float16)
+    transformer = Transformer2DModel.from_pretrained(args.pretrained_model_name_or_path, subfolder="transformer", torch_dtype=weight_dtype)
 
     # freeze parameters of models to save more memory
-    transformer.requires_grad_(False)
-    vae.requires_grad_(False)
-    text_encoder.requires_grad_(False)
-
+    transformer.requires_grad_(False)    
+    
     # For mixed precision training we cast all non-trainable weigths (vae, non-lora text_encoder and non-lora transformer) to half-precision
     # as these weights are only used for inference, keeping weights in full precision is not required.
     weight_dtype = torch.float32
@@ -509,15 +532,28 @@ def main():
             "linear_1",
             "linear_2",
             # "scale_shift_table",      # not available due to the implementation in huggingface/peft, working on it.
-        ]
+        ],
+        use_dora = args.use_dora,
+        use_rslora = args.use_rslora
     )
 
     # Move transformer, vae and text_encoder to device and cast to weight_dtype
-    transformer.to(accelerator.device, dtype=weight_dtype)
-    vae.to(accelerator.device, dtype=weight_dtype)
-    text_encoder.to(accelerator.device, dtype=weight_dtype)
+    transformer.to(accelerator.device)
+    
+    def cast_training_params(model: Union[torch.nn.Module, List[torch.nn.Module]], dtype=torch.float32):
+        if not isinstance(model, list):
+            model = [model]
+        for m in model:
+            for param in m.parameters():
+                # only upcast trainable parameters into fp32
+                if param.requires_grad:
+                    param.data = param.to(dtype)
 
     transformer = get_peft_model(transformer, lora_config)
+    if args.mixed_precision == "fp16":
+        # only upcast trainable parameters (LoRA) into fp32
+        cast_training_params(transformer, dtype=torch.float32)
+
     transformer.print_trainable_parameters()
 
     # 10. Handle saving and loading of checkpoints
@@ -907,7 +943,7 @@ def main():
                 break
 
         if accelerator.is_main_process:
-            if args.validation_prompt is not None and epoch % args.validation_epochs == 0:
+            if args.validation_prompt is not None and (epoch + 1) % args.validation_epochs == 0:
                 logger.info(
                     f"Running validation... \n Generating {args.num_validation_images} images with prompt:"
                     f" {args.validation_prompt}."
@@ -983,9 +1019,8 @@ def main():
     if args.seed is not None:
         generator = generator.manual_seed(args.seed)
     images = []
-    with torch.autocast("cuda", dtype=weight_dtype):
-        for _ in range(args.num_validation_images):
-            images.append(pipeline(args.validation_prompt, num_inference_steps=20, generator=generator).images[0])
+    for _ in range(args.num_validation_images):
+        images.append(pipeline(args.validation_prompt, num_inference_steps=20, generator=generator).images[0])
 
     if accelerator.is_main_process:
         for tracker in accelerator.trackers:
@@ -1007,4 +1042,4 @@ def main():
 
 
 if __name__ == "__main__":
-    main()
\ No newline at end of file
+    main()
